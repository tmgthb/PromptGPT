{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PromptGPT - Practical guide\n\n*Could a Large Language Model (LLM) handle the role of Prompt engineer?*\n\nRecently, Large Language Models (think GPT-4, etc) are beginning to beat benchmarks, which are difficult even for humans, such as official exams.\n\nIn this guide, I will introduce with a tutorial on: How to get started with PromptGPT. \n\n## PromptGPT\n\nPrompt GPT includes different types of re-usable Prompt Programs:\n\n1. A single prompt function to make all API calls to a LLM (GPT-4, Anthropic API, etc.) \n2. High-level \"cognitive functions\" to manage workflow of the PromptGPT, such as: Planning, Tasking, Execution, etc. \n3. \"Low level\" programs, such as saving & retrieving API calls / memories. \n\nPromptGPT Settings enable:\n* Define instructions for Prompt programs\n* Control safety: limit usage and prohibited tasks.\n\n## PromptGPT: Getting started\n\nLet's start by importing libraries. Make sure to add your token/key as an environmental variable","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade openai\nimport openai\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nopenai.api_key = user_secrets.get_secret(\"OPENAI_API_KEY\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-19T22:41:07.453870Z","iopub.execute_input":"2023-04-19T22:41:07.454602Z","iopub.status.idle":"2023-04-19T22:41:18.954936Z","shell.execute_reply.started":"2023-04-19T22:41:07.454557Z","shell.execute_reply":"2023-04-19T22:41:18.953484Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Requirement already satisfied: openai in /opt/conda/lib/python3.7/site-packages (0.27.4)\nRequirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.7/site-packages (from openai) (2.28.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from openai) (4.64.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from openai) (4.4.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from openai) (3.8.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20->openai) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20->openai) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20->openai) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20->openai) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->openai) (6.0.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->openai) (22.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->openai) (1.3.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->openai) (1.8.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->openai) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->openai) (1.3.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->openai) (4.0.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## PromptGPT: Settings\n\nPromptGPT Settings make it easy to steer PromptGPT behaviour. Each cognitive task uses its own function, which each use their own User Prompt and LLM system instruction. For example planning task is given different instruction, than one executing specific planned action. This enables functions to always use single function for the API call. This simplifies the overall Prompt Programming.\n\nAspects to define in Settings:\n* User input method\n* Context / Message lengths\n* Model(s) to use\n* Memory (filename)\n* Instructions: Prompt \n* System instructions: LLMs system instruction on how to handle the Prompt.\n","metadata":{}},{"cell_type":"code","source":"# SYSTEM SETTTINGS\n#prompt = input(\"Ask a question: \")\nprompt = \"Generate 1000 followers with zero budget.\" #prompt = \"Organize and execute a plan to generate 1000 followers in twitter without spending money in 5 steps.\"\nmax_length_input = 500\nexecution_length = 2000\nmodel_to_use = 'gpt-4'\n#model_to_use = 'gpt-3.5-turbo'\nfilepath = 'short_memory.txt'\nsystem_instruction = \"You are a world-class prompt engineer producing right prompts to LLM in JSON format only to support programming anything. Only one overall objective. Prompts solve correctly anything by mapping up to 5, short, precise, measurable task/sub-task descriptions, task output types & task role to perform them & necessary concrete resource and/or memory name & use only status messages: Completed, In progress, Error, Rework. Use only single world to define the resource: for example Twitter, Github, Medium. Add the task description specific output type.\"\ndefine_task = 'Print only the task name from the json: {}.'\ndefine_role = \"Print only the task role from the json: {}.\"\ndefine_resource = \"Print only the task resource from the json: {}.\"\ndefine_output_type = \"Print only the task resource from the json: {}.\"\ndefine_memory = \"Print only the task memory from the json: {}.\"\ndefine_task_system = f\"You are a task selector. You only print name of the task. You only pick one task, which is in the first task not completed yet and in status in progresss.\"\ndefine_role_system = f\"You are a role selector. You only print name of the role of the task. You only pick one role, which is in the first task not completed yet and in status in progresss.\" \ndefine_resource_system = f\"You are a resource selector. You only print resource of the task. You only pick one resource, which is in the first task not completed yet and in status in progresss.\" \ndefine_output_type_system = f\"You are an output_type selector. You only print output_type of the task. You only pick one output_type, which is in the first task not completed yet and in status in progresss.\" \ndefine_memory_system = f\"You are a task memory selector. You only print memory name of the task. You only pick one memory, which is in the first task not completed yet and in status in progresss. In case no memory required: print No memory\"\nexecution_script = f\"print only the solution as a json:\"\nexecution_system_instruction =  \"You are a world-class {} creating a specific answer to: {}. You use as resource: {} to produce {}. Limit is 300 characters\" ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T22:41:18.957486Z","iopub.execute_input":"2023-04-19T22:41:18.957857Z","iopub.status.idle":"2023-04-19T22:41:18.966330Z","shell.execute_reply.started":"2023-04-19T22:41:18.957820Z","shell.execute_reply":"2023-04-19T22:41:18.965035Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"**Prompt Programs**\n\nWe are now in the section, where the Prompt Programs reside. These are simply Python functions. The first function is the generic LLM API call. It works on any request, which makes simple to integrate inside any other function with specific type of cognitive purpose. \n\n* Generic gpt_program()-function handles all API calls to a LLM.\n* Cognitive skills-functions (Planning, Executing etc) steer the \"high-level\" workflow\n* Specialized skills handle resources: like managing memory","metadata":{}},{"cell_type":"code","source":"def gpt_program(gpt_program_input, gpt_program_system, gpt_model, max_length):\n    \"\"\"\n        ---Generic GPT Program---\n        gpt_program_input: what is the input?\n        gpt_program_system: What should gpt do?\n        gpt_model: What model to use?\n        max_length: What is the total length input + output?\n    \"\"\"\n    gpt_program_response = openai.ChatCompletion.create(\n        model = gpt_model,\n        messages = [\n            {\"role\": \"system\", \"content\": f\"{gpt_program_system}\"},\n            {\"role\": \"user\", \"content\": \"\"},\n            {\"role\": \"assistant\", \"content\": \"How can I help you?\"},\n            {\"role\": \"user\", \"content\": f\"{gpt_program_input}\"},\n        ],\n        max_tokens=max_length,\n        temperature=0)\n    gpt_program_output = gpt_program_response['choices'][0]['message']['content']\n    return gpt_program_output\n\n\ndef save_memory(memory_input, memory_file):\n    \"\"\"\n    ---SAVE MEMORY---\n    memory_input: What is text to be stored in the .txt-file?\n    memory_file: What variable is used to store the text?\n    \"\"\"\n    with open(memory_file, 'w') as f:\n        f.write(str(memory_input))\n    return memory_file\n\n\ndef get_memory(memory_file, memory):\n    \"\"\"\n    ---RETRIEVE MEMORY---\n    memory_file: What variable is used to store the text?\n    memory: What is the memory variable? \n    \"\"\"\n    with open(memory_file, \"r\") as f:\n        memory = f.read()\n    return memory \n\n\ndef get_task(define_task, define_task_system):\n    \"\"\"\n    ---GET TASK DESCRIPTION---\n    \"\"\"\n    actual_task = \"\"\n    define_task = define_task.format(memory)\n    actual_task = gpt_program(define_task, define_task_system, model_to_use, execution_length)\n    return actual_task\n\ndef get_role(define_role, define_role_system):\n    \"\"\"\n    ---GET TASK ROLE---\n    \"\"\"\n    actual_task_role = \"\"\n    define_role = define_role.format(memory)\n    actual_task_role = gpt_program(define_role, define_role_system, model_to_use, execution_length)\n    return actual_task_role\n\ndef get_resource(define_resource, define_resource_system):\n    \"\"\"\n    ---GET TASK RESOURCE---\n    \"\"\"\n    actual_task_resource = \"\"\n    define_resource = define_resource.format(memory)\n    actual_task_resource = gpt_program(define_resource, define_resource_system, model_to_use, execution_length)\n    return actual_task_resource\n\ndef get_output_type(define_output_type, define_output_type_system):\n    \"\"\"\n    ---GET TASK OUTPUT TYPE---\n    \"\"\"\n    actual_task_output_type = \"\"\n    define_output_type = define_output_type.format(memory)\n    actual_task_output_type = gpt_program(define_resource, define_output_type_system, model_to_use, execution_length)\n    return actual_task_output_type\n\ndef get_task_memory(define_memory, define_memory_system):\n    \"\"\"\n    ---GET TASK MEMORY---\n    \"\"\"\n    actual_task_memory = \"\"\n    define_memory = define_memory.format(memory)\n    actual_task_memory = gpt_program(define_memory, define_memory_system, model_to_use, execution_length)\n    return actual_task_memory\n\ndef execute(execution_script, execution_system_instruction):\n    \"\"\"\n    ---EXECUTE A TASK---\n    \"\"\"\n    execution_output = \"\"\n    execution_system_instruction= execution_system_instruction.format(task_role, task, task_resource, task_output_type)\n    execution_output = gpt_program(execution_script, execution_system_instruction, model_to_use, execution_length)\n    return execution_output\n","metadata":{"execution":{"iopub.status.busy":"2023-04-19T22:41:18.968185Z","iopub.execute_input":"2023-04-19T22:41:18.968664Z","iopub.status.idle":"2023-04-19T22:41:18.985012Z","shell.execute_reply.started":"2023-04-19T22:41:18.968618Z","shell.execute_reply":"2023-04-19T22:41:18.983816Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"**Planning**\n\nLet's now process the user input. Planning is first task. It uses the generic gpt_program()-function, but adds the Planning specific Instructions & System messages. The result is an execution plan for the user objective.","metadata":{}},{"cell_type":"code","source":"my_plan = gpt_program(prompt, system_instruction, model_to_use, max_length_input)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T22:41:18.988411Z","iopub.execute_input":"2023-04-19T22:41:18.988997Z","iopub.status.idle":"2023-04-19T22:41:57.875584Z","shell.execute_reply.started":"2023-04-19T22:41:18.988935Z","shell.execute_reply":"2023-04-19T22:41:57.874488Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"**Memory**\n\nI can now store this plan in a memory. We can use it as a short-term memory or as a long-term memory. The difference is token consumption. I could for example store information about a book permanently, but I may not want to store longer a specific plan for completing a specific task. I see this especially impacting to overall token consumption.\n\nLet's finally retrieve the memory from the short_memory.txt file and print it. We use JSON-format, which includes information such as:\n- Overall objective\n- Task list\n- Task description\n- Task role\n- Task resource\n- Task status\n\nThese different elements are useful: some include pre-defined options: In progress, Closed/Error/Rework, while for example Role and Task description is defined by LLM.","metadata":{}},{"cell_type":"code","source":"save_memory(my_plan, filepath)\nmemory = get_memory(filepath, '')\nprint(memory)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T22:41:57.876779Z","iopub.execute_input":"2023-04-19T22:41:57.877318Z","iopub.status.idle":"2023-04-19T22:41:57.884044Z","shell.execute_reply.started":"2023-04-19T22:41:57.877283Z","shell.execute_reply":"2023-04-19T22:41:57.882852Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"{\n  \"objective\": \"Generate 1000 followers with zero budget\",\n  \"tasks\": [\n    {\n      \"description\": \"Research effective strategies for organic growth on social media platforms\",\n      \"output_type\": \"List of strategies\",\n      \"role\": \"Researcher\",\n      \"resource\": \"Internet\",\n      \"status\": \"In progress\"\n    },\n    {\n      \"description\": \"Create high-quality, engaging content tailored to the target audience\",\n      \"output_type\": \"Content\",\n      \"role\": \"Content Creator\",\n      \"resource\": \"Creativity\",\n      \"status\": \"In progress\"\n    },\n    {\n      \"description\": \"Optimize social media profiles for discoverability and engagement\",\n      \"output_type\": \"Optimized profiles\",\n      \"role\": \"Social Media Manager\",\n      \"resource\": \"Social Media Platforms\",\n      \"status\": \"In progress\"\n    },\n    {\n      \"description\": \"Implement growth strategies and monitor progress\",\n      \"output_type\": \"Growth metrics\",\n      \"role\": \"Social Media Manager\",\n      \"resource\": \"Analytics\",\n      \"status\": \"In progress\"\n    },\n    {\n      \"description\": \"Adjust strategies based on performance and feedback\",\n      \"output_type\": \"Improved strategies\",\n      \"role\": \"Social Media Manager\",\n      \"resource\": \"Analytics\",\n      \"status\": \"Rework\"\n    }\n  ]\n}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Task definition**\n\nPromptGPT uses pre-defined functions to look up from the overall plan, the first available task not yet completed. It will then retrieve the respective task description, role and resources for it. This way, makes the PromptGPT very autonomous. I can simply send it to look up the current task list and it can figure out right parameters for executing it.","metadata":{}},{"cell_type":"code","source":"task = get_task(define_task, define_task_system)\ntask_role = get_role(define_role, define_role_system)\ntask_resource = get_resource(define_resource, define_resource_system)\ntask_output_type = get_output_type(define_output_type, define_output_type_system) \n#task_memory = get_memory(define_memory, define_memory_system)\nprint(task + \"/ \" + task_role + \"/ \" + task_resource + \"/ \" + task_output_type)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T22:41:57.885675Z","iopub.execute_input":"2023-04-19T22:41:57.886029Z","iopub.status.idle":"2023-04-19T22:42:06.576806Z","shell.execute_reply.started":"2023-04-19T22:41:57.885994Z","shell.execute_reply":"2023-04-19T22:42:06.575518Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Research effective strategies for organic growth on social media platforms/ Researcher/ Internet/ Please provide the JSON data containing the task resource and its status, so I can help you with your request.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Execution**\n\nPromptGPT executes tasks using the task description, task role and task resources defined in the previous step. The","metadata":{}},{"cell_type":"code","source":"# EXECUTION\nexecuted_json = execute(execution_script, execution_system_instruction)\nprint(executed_json)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T22:42:06.577939Z","iopub.execute_input":"2023-04-19T22:42:06.578782Z","iopub.status.idle":"2023-04-19T22:42:39.544347Z","shell.execute_reply.started":"2023-04-19T22:42:06.578748Z","shell.execute_reply":"2023-04-19T22:42:39.543315Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"{\n  \"strategies\": [\n    {\n      \"name\": \"Consistent Posting\",\n      \"description\": \"Post regularly to maintain audience engagement and increase visibility.\"\n    },\n    {\n      \"name\": \"Engaging Content\",\n      \"description\": \"Create high-quality, relevant, and engaging content that resonates with your target audience.\"\n    },\n    {\n      \"name\": \"Hashtags\",\n      \"description\": \"Use relevant and trending hashtags to increase the discoverability of your content.\"\n    },\n    {\n      \"name\": \"Collaborations\",\n      \"description\": \"Partner with influencers and other brands to expand your reach and credibility.\"\n    },\n    {\n      \"name\": \"User-Generated Content\",\n      \"description\": \"Encourage and share content created by your audience to foster community and trust.\"\n    },\n    {\n      \"name\": \"Contests and Giveaways\",\n      \"description\": \"Host contests and giveaways to increase engagement and attract new followers.\"\n    },\n    {\n      \"name\": \"Analytics\",\n      \"description\": \"Monitor and analyze your social media performance to optimize your strategy and content.\"\n    }\n  ]\n}\n","output_type":"stream"}]}]}
